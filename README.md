# GPT-2 Language Model Analysis â€“ ShadowFox AI/ML Internship (Advanced Task)

ğŸš€ **Project Level**: Advanced  
ğŸ“ **Internship**: ShadowFox AI/ML Internship  
ğŸ¯ **Task**: Implement a Language Model (LM) and analyze its performance

---

## ğŸ§  Project Overview

This project explores the capabilities of **GPT-2**, a powerful pre-trained language model developed by OpenAI and accessed via HuggingFace's `transformers` library. The goal is to analyze how well GPT-2 can generate human-like text based on various types of prompts.

As part of the **Advanced Task** for the ShadowFox AI/ML Internship, this notebook demonstrates implementation, performance testing, and visualization of a real-world language model.

---

## ğŸ› ï¸ Tools & Technologies Used

- Python
- Google Colab
- HuggingFace Transformers (`gpt2`)
- PyTorch
- Matplotlib & Seaborn

---

## ğŸ“¦ Features

- Load and configure GPT-2 model
- Generate text from custom prompts
- Explore multiple domains (stories, coding, science, diet, etc.)
- Visualize prompt token lengths
- Analyze LM performance with research-based observations

---

## ğŸ§ª How It Works

1. **Input**: A text prompt (e.g., *"Once upon a time..."*)
2. **Model**: GPT-2 generates the next sequence of words
3. **Output**: A complete paragraph of AI-generated text

Example:

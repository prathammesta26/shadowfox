# GPT-2 Language Model Analysis – ShadowFox AI/ML Internship (Advanced Task)

🚀 **Project Level**: Advanced  
🎓 **Internship**: ShadowFox AI/ML Internship  
🎯 **Task**: Implement a Language Model (LM) and analyze its performance

---

## 🧠 Project Overview

This project explores the capabilities of **GPT-2**, a powerful pre-trained language model developed by OpenAI and accessed via HuggingFace's `transformers` library. The goal is to analyze how well GPT-2 can generate human-like text based on various types of prompts.

As part of the **Advanced Task** for the ShadowFox AI/ML Internship, this notebook demonstrates implementation, performance testing, and visualization of a real-world language model.

---

## 🛠️ Tools & Technologies Used

- Python
- Google Colab
- HuggingFace Transformers (`gpt2`)
- PyTorch
- Matplotlib & Seaborn

---

## 📦 Features

- Load and configure GPT-2 model
- Generate text from custom prompts
- Explore multiple domains (stories, coding, science, diet, etc.)
- Visualize prompt token lengths
- Analyze LM performance with research-based observations

---

## 🧪 How It Works

1. **Input**: A text prompt (e.g., *"Once upon a time..."*)
2. **Model**: GPT-2 generates the next sequence of words
3. **Output**: A complete paragraph of AI-generated text

Example:

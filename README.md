# GPT-2 Language Model Analysis – ShadowFox AI/ML Internship (Advanced Task)

🚀 **Project Level**: Advanced  
🎓 **Internship**: ShadowFox AI/ML Internship  
🎯 **Task**: Implement a Language Model (LM) and analyze its performance

---

## 🧠 Project Overview

This project explores the capabilities of **GPT-2**, a powerful pre-trained language model developed by OpenAI and accessed via HuggingFace's `transformers` library. The goal is to analyze how well GPT-2 can generate human-like text based on various types of prompts.

As part of the **Advanced Task** for the ShadowFox AI/ML Internship, this notebook demonstrates implementation, performance testing, and visualization of a real-world language model.

---

## 🛠️ Tools & Technologies Used

- Python
- Google Colab
- HuggingFace Transformers (`gpt2`)
- PyTorch
- Matplotlib & Seaborn


---

## 📊 Visualizations

- Bar chart showing token length of input prompts
- Helps understand complexity and how much context the model processes

---

## 🔍 Research Questions Explored

- How well does GPT-2 handle different domains (stories vs. technical)?
- Does it maintain context in long text?
- Can GPT-2 provide coherent and logical completions?

---

## 📚 Conclusion

GPT-2 demonstrates strong text generation capabilities, especially in general domains. It understands prompts and generates fluent, human-like responses. However, it may lose coherence in long-form or domain-specific content without fine-tuning.

---

## 🏁 Future Scope

- Try domain-specific LMs (like BioBERT or LegalBERT)
- Explore fine-tuning GPT-2 on custom datasets
- Use GPT-2 for next-word prediction or chatbot development

---

## 🧾 Proof of Work

- ✅ Jupyter Notebook (.ipynb) included
- ✅ Video explanation submitted
- ✅ Project shared on LinkedIn with GitHub link

---

## 📂 Project Structure


---

## 📦 Features

- Load and configure GPT-2 model
- Generate text from custom prompts
- Explore multiple domains (stories, coding, science, diet, etc.)
- Visualize prompt token lengths
- Analyze LM performance with research-based observations

---

## 🧪 How It Works

1. **Input**: A text prompt (e.g., *"Once upon a time..."*)
2. **Model**: GPT-2 generates the next sequence of words
3. **Output**: A complete paragraph of AI-generated text

Example:
